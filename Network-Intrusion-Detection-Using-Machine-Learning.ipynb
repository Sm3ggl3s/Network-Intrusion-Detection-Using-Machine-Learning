{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7f741d",
   "metadata": {},
   "source": [
    "# Network Intrusion Detection on DataSense: CIC IIoT 2025 (1-Second Windows)\n",
    "\n",
    "This notebook builds a **modular and share-safe** ML pipeline for the **CIC IIoT 2025** (DataSense) dataset.  \n",
    "It supports **1–10 s** window CSVs, synced sensor+network features, and both **binary** and **multiclass** tasks.\n",
    "\n",
    "## ⚙️ Pipeline Overview\n",
    "1. Load and combine the 1-second attack & benign CSVs, then **shuffle** the rows.\n",
    "2. Do **light cleaning** and prepare features.\n",
    "3. Use **ANOVA (f-test)** to rank features and select the most informative ones.\n",
    "4. Use **Stratified K-Fold** cross-validation for robust evaluation.\n",
    "5. Train baseline models:\n",
    "   - Logistic Regression  \n",
    "   - SVM (RBF)  \n",
    "   - Random Forest  \n",
    "6. Train additional models:\n",
    "   - K-Means clustering (unsupervised ⇒ mapped to labels)  \n",
    "   - K-Nearest Neighbors (KNN)  \n",
    "   - LightGBM (if installed)  \n",
    "   - XGBoost (if installed)\n",
    "7. Compare models using:\n",
    "   - Accuracy  \n",
    "   - **Macro F1-score** (primary)  \n",
    "   - Full classification report  \n",
    "   - Confusion matrix for the best model\n",
    "\n",
    "\n",
    "**Dependencies:** `python-dotenv`, `pandas`, `numpy`, `scikit-learn`, `matplotlib`, `seaborn`, `joblib`, `lightgbm`, `imbalanced-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19879c-6ad7-4549-bb28-6686365319b5",
   "metadata": {},
   "source": [
    "### Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0824cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Imports & configuration\n",
    "import os\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d5c87",
   "metadata": {},
   "source": [
    "### 1. Load & Combine 1-Second CSVs (Shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02c5302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base data path: /home/jovyan/Notebooks/DATA\n",
      "Attack CSV: /home/jovyan/Notebooks/DATA/attack_data/attack_samples_1sec.csv\n",
      "Benign CSV: /home/jovyan/Notebooks/DATA/benign_data/benign_samples_1sec.csv\n",
      "Attack shape: (90391, 94)\n",
      "Benign shape: (136800, 94)\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(\"/home/jovyan/Notebooks/.env\")\n",
    "base_path = os.getenv(\"DATA_PATH\")\n",
    "# Sanity check\n",
    "print(\"Base data path:\", base_path)\n",
    "\n",
    "attack_file = \"attack_samples_1sec.csv\"\n",
    "benign_file = \"benign_samples_1sec.csv\"\n",
    "\n",
    "attack_path = os.path.join(base_path, \"attack_data\", attack_file)\n",
    "benign_path = os.path.join(base_path, \"benign_data\", benign_file)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Attack CSV:\", attack_path)\n",
    "print(\"Benign CSV:\", benign_path)\n",
    "\n",
    "# Loading CSVs (Preview mode)\n",
    "df_attack = pd.read_csv(attack_path, low_memory=False)\n",
    "df_benign = pd.read_csv(benign_path, low_memory=False)\n",
    "\n",
    "# Check if loaded (print shape)\n",
    "print(\"Attack shape:\", df_attack.shape)\n",
    "print(\"Benign shape:\", df_benign.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0939f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (227191, 95)\n",
      "         device_name         device_mac  \\\n",
      "0         plug-flame  d4:a6:51:20:91:f7   \n",
      "1  ultrasonic-sensor  08:b6:1f:82:ee:c4   \n",
      "2   plug-all-sensors  d4:a6:51:82:98:a8   \n",
      "3             router  28:87:ba:bd:c6:6c   \n",
      "4   vibration-sensor  08:b6:1f:82:27:d0   \n",
      "\n",
      "                                         label_full  label1  label2  \\\n",
      "0                             benign_whole-network3  benign  benign   \n",
      "1                             benign_whole-network3  benign  benign   \n",
      "2  attack_recon_host-disc-udp-ping_plug-all-sensors  attack   recon   \n",
      "3            attack_mitm_ip-spoofing_router--switch  attack    mitm   \n",
      "4              attack_recon_port-scan_whole-network  attack   recon   \n",
      "\n",
      "               label3                    label4  \\\n",
      "0              benign                    benign   \n",
      "1              benign                    benign   \n",
      "2  host-disc-udp-ping  recon_host-disc-udp-ping   \n",
      "3         ip-spoofing          mitm_ip-spoofing   \n",
      "4           port-scan           recon_port-scan   \n",
      "\n",
      "                                           timestamp  \\\n",
      "0  2025-09-09T14:16:38.400000Z_2025-09-09T14:16:3...   \n",
      "1  2025-09-09T14:41:23.400000Z_2025-09-09T14:41:2...   \n",
      "2  2025-01-16T13:48:12.048000Z_2025-01-16T13:48:1...   \n",
      "3  2025-02-14T01:21:47.621000Z_2025-02-14T01:21:4...   \n",
      "4  2025-01-15T13:05:02.516000Z_2025-01-15T13:05:0...   \n",
      "\n",
      "               timestamp_start                timestamp_end  ...  \\\n",
      "0  2025-09-09T14:16:38.400000Z  2025-09-09T14:16:39.400000Z  ...   \n",
      "1  2025-09-09T14:41:23.400000Z  2025-09-09T14:41:24.400000Z  ...   \n",
      "2  2025-01-16T13:48:12.048000Z  2025-01-16T13:48:13.048000Z  ...   \n",
      "3  2025-02-14T01:21:47.621000Z  2025-02-14T01:21:48.621000Z  ...   \n",
      "4  2025-01-15T13:05:02.516000Z  2025-01-15T13:05:03.516000Z  ...   \n",
      "\n",
      "   network_time-delta_std_deviation  network_ttl_avg  network_ttl_max  \\\n",
      "0                          0.000000         0.000000              0.0   \n",
      "1                          0.016731       159.500000            255.0   \n",
      "2                          0.003678       151.476496            255.0   \n",
      "3                          0.000077        63.995786             64.0   \n",
      "4                          0.017448       159.500000            255.0   \n",
      "\n",
      "   network_ttl_min network_ttl_std_deviation  network_window-size_avg  \\\n",
      "0              0.0                  0.000000                 0.000000   \n",
      "1             64.0                 95.500000             34826.000000   \n",
      "2             37.0                103.616617              1972.000000   \n",
      "3              1.0                  0.480594              6947.333333   \n",
      "4             64.0                 95.500000             34454.500000   \n",
      "\n",
      "   network_window-size_max  network_window-size_min  \\\n",
      "0                      0.0                      0.0   \n",
      "1                  64062.0                   5590.0   \n",
      "2                   2920.0                   1024.0   \n",
      "3                  15008.0                    282.0   \n",
      "4                  64075.0                   4834.0   \n",
      "\n",
      "   network_window-size_std_deviation  source_file  \n",
      "0                           0.000000       benign  \n",
      "1                       29236.000000       benign  \n",
      "2                         948.000000       attack  \n",
      "3                        6874.529855       attack  \n",
      "4                       29620.500000       attack  \n",
      "\n",
      "[5 rows x 95 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add source file column to identify the origin of each sample\n",
    "df_attack[\"source_file\"] = \"attack\"\n",
    "df_benign[\"source_file\"] = \"benign\"\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([df_attack, df_benign], ignore_index=True)\n",
    "\n",
    "# shuffle the combined dataset\n",
    "df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# Check combined shape\n",
    "print(\"Combined shape:\", df.shape)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c371f",
   "metadata": {},
   "source": [
    "### 2. Clean & Prepare Features\n",
    "\n",
    "**Goals:**\n",
    "- Choose the target label column: we’ll use `label2` (attack category).\n",
    "- Drop raw identifier / list-like columns (IPs, ports, MACs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0253aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (227191, 71)\n",
      "Target distribution:\n",
      "label2\n",
      "benign        0.602137\n",
      "recon         0.148104\n",
      "dos           0.081077\n",
      "ddos          0.079475\n",
      "mitm          0.035486\n",
      "malware       0.033192\n",
      "web           0.012307\n",
      "bruteforce    0.008222\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "TARGET = \"label2\" # Target column name\n",
    "\n",
    "# checking it exists in the dataframe\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET}' not found in the dataframe.\")\n",
    "\n",
    "# Drop raw identifiers / list-like text columns, which are not useful for modeling\n",
    "drop_cols = [\n",
    "    \"device_mac\",\n",
    "    \"network_ips_all\",\"network_ips_dst\",\"network_ips_src\",\n",
    "    \"network_macs_all\",\"network_macs_dst\",\"network_macs_src\",\n",
    "    \"network_ports_all\",\"network_ports_dst\",\"network_ports_src\",\n",
    "    \"network_protocols_all\",\"network_protocols_dst\",\"network_protocols_src\",\n",
    "]\n",
    "drop_cols = [c for c in drop_cols if c in df.columns]\n",
    "df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Set up features and target\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype(str)  # Ensure target is string type for classification\n",
    "\n",
    "# Drop any non-numeric columns in X\n",
    "obj_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "X = X.drop(columns=obj_cols)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126f26d",
   "metadata": {},
   "source": [
    "### 3. ANOVA (f-test) Feature Ranking\n",
    "\n",
    "- Uses `SelectKBest(f_classif)` to compute an F-score for each feature.\n",
    "- Higher F-score = stronger relationship between that feature and the class labels.\n",
    "- This is a simple, fast filter method to reduce noise and focus on informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001da998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 30 features by ANOVA F-score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "network_mss_max                      30107.385961\n",
       "network_mss_avg                      29926.800948\n",
       "network_mss_min                      29334.814214\n",
       "network_packets_all_count            25808.666533\n",
       "network_packets_dst_count            25549.298304\n",
       "network_ips_all_count                23450.157106\n",
       "network_ips_dst_count                23446.285168\n",
       "network_ports_all_count              15324.252625\n",
       "network_macs_all_count               14823.585131\n",
       "network_macs_dst_count               13972.036543\n",
       "network_macs_src_count               13972.036543\n",
       "network_ips_src_count                13421.363712\n",
       "network_ports_src_count              13395.086059\n",
       "network_protocols_dst_count          12156.897017\n",
       "network_protocols_src_count          10944.116919\n",
       "network_protocols_all_count           9474.356954\n",
       "network_header-length_min             9243.443779\n",
       "network_header-length_max             9243.104091\n",
       "network_header-length_avg             9241.169235\n",
       "network_fragmentation-score           8281.453361\n",
       "network_fragmented-packets            8146.966021\n",
       "network_packet-size_avg               6698.588661\n",
       "network_payload-length_avg            6648.911245\n",
       "network_ip-length_avg                 6104.526654\n",
       "network_ip-flags_avg                  5603.152921\n",
       "network_packet-size_max               4957.302171\n",
       "network_ip-length_max                 4900.894649\n",
       "network_payload-length_max            4860.330006\n",
       "network_packet-size_std_deviation     4348.703840\n",
       "network_ip-length_std_deviation       4005.363265\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for modeling: ['network_mss_max', 'network_mss_avg', 'network_mss_min', 'network_packets_all_count', 'network_packets_dst_count', 'network_ips_all_count', 'network_ips_dst_count', 'network_ports_all_count', 'network_macs_all_count', 'network_macs_dst_count', 'network_macs_src_count', 'network_ips_src_count', 'network_ports_src_count', 'network_protocols_dst_count', 'network_protocols_src_count', 'network_protocols_all_count', 'network_header-length_min', 'network_header-length_max', 'network_header-length_avg', 'network_fragmentation-score', 'network_fragmented-packets', 'network_packet-size_avg', 'network_payload-length_avg', 'network_ip-length_avg', 'network_ip-flags_avg', 'network_packet-size_max', 'network_ip-length_max', 'network_payload-length_max', 'network_packet-size_std_deviation', 'network_ip-length_std_deviation']\n",
      "\n",
      "Final feature matrix shape: (227191, 30)\n"
     ]
    }
   ],
   "source": [
    "# Double-check imports\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values with median \n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = pd.DataFrame(imp.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Computer F-scores\n",
    "selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "selector.fit(X_imputed, y)\n",
    "\n",
    "f_scores = pd.Series(selector.scores_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Select top features (e.g., top 30)\n",
    "TOP_K = 30  # Select top 30 features\n",
    "print(f\"\\nTop {TOP_K} features by ANOVA F-score:\")\n",
    "display(f_scores.head(TOP_K))\n",
    "\n",
    "selected_features = f_scores.head(TOP_K).index.tolist()\n",
    "print(\"Selected features for modeling:\", selected_features)\n",
    "\n",
    "# Final modeling matrix\n",
    "X_selected = X_imputed[selected_features].copy()\n",
    "y_selected = y.copy()\n",
    "print(\"\\nFinal feature matrix shape:\", X_selected.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cc5c8",
   "metadata": {},
   "source": [
    "### 4. Use Stratified K-Fold cross-validation for robust evaluation.\n",
    "\n",
    "We use **10-fold stratified cross-validation** (9:1 ratio per fold) so that:\n",
    "- each model is trained on ~90% of the data and validated on ~10%,\n",
    "- every sample appears in a validation fold exactly once,\n",
    "- class proportions are preserved in each fold (StratifiedKFold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f087bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Stratified 10-Fold Cross-Validation\n"
     ]
    }
   ],
   "source": [
    "# Double check imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set up Stratified K-Fold\n",
    "N_SPLITS = 10 # 9-1 train-test split\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"\\nPerforming Stratified {N_SPLITS}-Fold Cross-Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d935dc",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "We’ll reuse these for all models.\n",
    "\n",
    "We will define:\n",
    "\n",
    "- `cross_val_oof_predict`:\n",
    "  - runs K-fold training,\n",
    "  - stores predictions for each sample from the fold where it was *not* used for training.\n",
    "- `eval_report`:\n",
    "  - prints Accuracy & **Macro-F1**,\n",
    "  - shows a full classification report,\n",
    "  - returns a confusion matrix DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d32e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_oof_predict(estimator, X, y, skf):\n",
    "    '''\n",
    "    Perform out-of-fold predictions using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - estimator: The machine learning model to train.\n",
    "    - X: Feature matrix.\n",
    "    - y: Target vector.\n",
    "    - skf: StratifiedKFold cross-validator.\n",
    "\n",
    "    Returns:\n",
    "    - oof_preds: Out-of-fold predictions.\n",
    "    '''\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        estimator.fit(X_train, y_train)\n",
    "        oof_preds[test_idx] = estimator.predict(X_test)\n",
    "        print(f\"    Completed Fold {fold + 1}/{N_SPLITS}\")\n",
    "\n",
    "    return oof_preds\n",
    "\n",
    "def eval_report(y_true, y_pred, label=\"Model\"):\n",
    "    '''\n",
    "    Print evaluation metrics for the model.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: True labels.\n",
    "    - y_pred: Predicted labels.\n",
    "    - label: Model label for reporting.\n",
    "    '''\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n=== Evaluation Report for {label} ===:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\\n\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=sorted(y_true.unique()))\n",
    "    cm_df = pd.DataFrame(cm, index=sorted(y_true.unique()), columns=sorted(y_true.unique()))\n",
    "    return {\"accuracy\": acc, \"macro_f1\": macro_f1, \"cm\": cm_df}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
